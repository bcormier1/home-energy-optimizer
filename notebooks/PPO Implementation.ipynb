{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9289f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "parent_dir = os.getcwd()\n",
    "path = os.path.dirname(parent_dir)\n",
    "sys.path.append(path)\n",
    "\n",
    "from gym_homer.envs.test_env_v0 import HomerEnv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import gym\n",
    "from gym import spaces, wrappers\n",
    "import envpool\n",
    "\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer, AsyncCollector\n",
    "from tianshou.env import DummyVectorEnv, SubprocVectorEnv, ShmemVectorEnv\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.trainer import onpolicy_trainer\n",
    "from tianshou.utils.net.common import ActorCritic, Net\n",
    "from tianshou.utils.net.discrete import Actor, Critic\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "379f1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = pd.read_csv(path+\"/test_env_data.csv\", index_col=False).fillna(0)\n",
    "pd.DataFrame(data)\n",
    "\n",
    "vectorised = False\n",
    "\n",
    "if vectorised:\n",
    "    n_train_envs = 4\n",
    "    n_test_envs = 4\n",
    "\n",
    "    env = HomerEnv(data=data, start_soc='empty')\n",
    "    train_envs = ShmemVectorEnv([lambda: HomerEnv(data=data, start_soc='empty') for _ in range(n_train_envs)])\n",
    "    test_envs = ShmemVectorEnv([lambda: HomerEnv(data=data, start_soc='empty') for _ in range(n_test_envs)])\n",
    "    \n",
    "else:\n",
    "    n_train_envs = 8\n",
    "    n_test_envs = 8\n",
    "\n",
    "    env = HomerEnv(data=data, start_soc='empty')\n",
    "    train_envs = SubprocVectorEnv([lambda: HomerEnv(data=data, start_soc='empty') for _ in range(n_train_envs)])\n",
    "    test_envs = SubprocVectorEnv([lambda: HomerEnv(data=data, start_soc='empty') for _ in range(n_test_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b67b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net is the shared head of the actor and the critic\n",
    "hidden_sizes = [64,64]\n",
    "lr_optimizer = 1e-4\n",
    "\n",
    "net = Net(\n",
    "    env.observation_space.shape, \n",
    "    hidden_sizes=hidden_sizes, \n",
    "    device=device\n",
    "    )\n",
    "\n",
    "actor = Actor(net, env.action_space.n, device=device).to(device)\n",
    "\n",
    "critic = Critic(net, device=device).to(device)\n",
    "actor_critic = ActorCritic(actor, critic)\n",
    "\n",
    "# optimizer of the actor and the critic\n",
    "optim = torch.optim.Adam(actor_critic.parameters(), lr=lr_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b7941a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since environment action space is discrete \n",
    "dist = torch.distributions.Categorical\n",
    "policy = PPOPolicy(\n",
    "    actor, \n",
    "    critic, \n",
    "    optim, \n",
    "    dist, \n",
    "    action_space=env.action_space, \n",
    "    deterministic_eval=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3661bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorised = False\n",
    "\n",
    "if vectorised:\n",
    "    train_collector = AsyncCollector(policy, train_envs, VectorReplayBuffer(20000, len(train_envs)))\n",
    "    test_collector = AsyncCollector(policy, test_envs)\n",
    "else:\n",
    "    train_collector = Collector(policy, train_envs, VectorReplayBuffer(20000, len(train_envs)))\n",
    "    test_collector = Collector(policy, test_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00afb50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 6000it [00:00, 7810.36it/s, env_step=6000, len=11, loss=1.799, loss/clip=-0.006, loss/ent=1.094, loss/vf=3.632, n/ep=184, n/st=2000, rew=-2.58]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 6000it [00:00, 7988.76it/s, env_step=12000, len=11, loss=0.707, loss/clip=-0.005, loss/ent=0.995, loss/vf=1.442, n/ep=184, n/st=2000, rew=-2.11]                                                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 6000it [00:00, 7964.16it/s, env_step=18000, len=11, loss=0.426, loss/clip=-0.010, loss/ent=0.930, loss/vf=0.892, n/ep=184, n/st=2000, rew=-1.95]                                                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 6000it [00:00, 7971.63it/s, env_step=24000, len=11, loss=0.439, loss/clip=-0.008, loss/ent=0.924, loss/vf=0.913, n/ep=176, n/st=2000, rew=-1.52]                                                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 6000it [00:00, 7967.53it/s, env_step=30000, len=11, loss=0.612, loss/clip=-0.010, loss/ent=0.855, loss/vf=1.260, n/ep=176, n/st=2000, rew=-0.59]                                                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 1.750000 ± 0.000000, best_reward: 1.750000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 6000it [00:00, 7989.74it/s, env_step=36000, len=11, loss=0.642, loss/clip=-0.016, loss/ent=0.688, loss/vf=1.330, n/ep=184, n/st=2000, rew=0.12]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 1.750000 ± 0.000000, best_reward: 1.750000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 6000it [00:00, 7972.68it/s, env_step=42000, len=11, loss=0.419, loss/clip=-0.011, loss/ent=0.467, loss/vf=0.868, n/ep=184, n/st=2000, rew=1.09]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 1.750000 ± 0.000000, best_reward: 1.750000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 6000it [00:00, 7786.36it/s, env_step=48000, len=11, loss=0.273, loss/clip=-0.007, loss/ent=0.307, loss/vf=0.566, n/ep=184, n/st=2000, rew=1.32]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 1.750000 ± 0.000000, best_reward: 1.750000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 6000it [00:00, 7983.85it/s, env_step=54000, len=11, loss=0.108, loss/clip=-0.004, loss/ent=0.237, loss/vf=0.228, n/ep=184, n/st=2000, rew=1.56]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 1.750000 ± 0.000000, best_reward: 1.750000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 6000it [00:00, 7942.38it/s, env_step=60000, len=11, loss=0.068, loss/clip=-0.003, loss/ent=0.185, loss/vf=0.145, n/ep=176, n/st=2000, rew=1.64]                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 1.750000 ± 0.000000, best_reward: 1.750000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_steps = 5000\n",
    "n_max_epochs = 10\n",
    "rep_per_collector = 10\n",
    "eps_per_test = 10\n",
    "batch_size = 256\n",
    "n_steps_per_collect = 2000\n",
    "reward_stop = 4\n",
    "\n",
    "result = onpolicy_trainer(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    max_epoch=n_max_epochs,\n",
    "    step_per_epoch=n_steps,\n",
    "    repeat_per_collect=rep_per_collector,\n",
    "    episode_per_test=eps_per_test,\n",
    "    batch_size=batch_size,\n",
    "    step_per_collect= n_steps_per_collect,\n",
    "    stop_fn=lambda mean_reward: mean_reward >= reward_stop,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "784f2421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duration': '10.43s', 'train_time/model': '2.80s', 'test_step': 1210, 'test_episode': 110, 'test_time': '0.21s', 'test_speed': '5696.36 step/s', 'best_reward': 1.7499999906867743, 'best_result': '1.75 ± 0.00', 'train_step': 60000, 'train_episode': 5452, 'train_time/collector': '7.42s', 'train_speed': '5871.46 step/s'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's watch its performance!\n",
    "policy.eval()\n",
    "result = test_collector.collect(n_episode=10, render=False)\n",
    "print(\"Final reward: {}, length: {}\".format(result[\"rews\"].mean(), result[\"lens\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b80808",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a863a74765bf0c3fe50d1b3d9f296fe0cd846a803cf5e7d56750d3cf2026663d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
