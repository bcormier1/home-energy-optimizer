{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9289f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "parent_dir = os.getcwd()\n",
    "path = os.path.dirname(parent_dir)\n",
    "sys.path.append(path)\n",
    "\n",
    "from gym_homer.envs.test_env_v0 import HomerEnv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gym, torch, numpy as np, torch.nn as nn\n",
    "from tianshou.utils import WandbLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "from gym import spaces, wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25da4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_x</th>\n",
       "      <th>time_y</th>\n",
       "      <th>weekend</th>\n",
       "      <th>month_x</th>\n",
       "      <th>month_y</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>region_3</th>\n",
       "      <th>solar</th>\n",
       "      <th>loads</th>\n",
       "      <th>import_tariff</th>\n",
       "      <th>export_tariff</th>\n",
       "      <th>max_d</th>\n",
       "      <th>max_c</th>\n",
       "      <th>soc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.866</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.866</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.866</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.866</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    time_x  time_y  weekend  month_x  month_y  region_1  region_2  region_3  \\\n",
       "0    0.000   1.000        0      0.0      1.0         0         0         0   \n",
       "1    0.500   0.866        0      0.0      1.0         0         0         0   \n",
       "2    0.866   0.500        0      0.0      1.0         0         0         0   \n",
       "3    1.000   0.000        0      0.0      1.0         0         0         0   \n",
       "4    0.866  -0.500        0      0.0      1.0         0         0         0   \n",
       "5    0.500  -0.866        0      0.0      1.0         0         0         0   \n",
       "6    0.000  -1.000        0      0.0      1.0         0         0         0   \n",
       "7   -0.500  -0.866        0      0.0      1.0         0         0         0   \n",
       "8   -0.866  -0.500        0      0.0      1.0         0         0         0   \n",
       "9   -1.000  -0.000        0      0.0      1.0         0         0         0   \n",
       "10  -0.866   0.500        0      0.0      1.0         0         0         0   \n",
       "11  -0.500   0.866        0      0.0      1.0         0         0         0   \n",
       "\n",
       "    solar  loads  import_tariff  export_tariff  max_d  max_c  soc  \n",
       "0       0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "1      -2    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "2      -2    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "3       0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "4       0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "5       0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "6       0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "7       0    0.5           2.00            2.0    0.0    0.0  0.0  \n",
       "8       0    0.5           2.00            2.0    0.0    0.0  0.0  \n",
       "9       0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "10      0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "11      0    0.5           0.05            0.0    0.0    0.0  0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "data = pd.read_csv(path+\"/test_env_data.csv\", index_col=False).fillna(0)\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7baa9849-98c7-4863-ae11-abe60bdeaa4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#task = 'HomerEnv-v0'\\nenv = HomerEnv(data=data, start_soc='random')\\nlr, epoch, batch_size = 1e-3, 20, 64\\ntrain_num, test_num = 10, 100\\ngamma, n_step, target_freq = 0.9, 3, 320\\nbuffer_size = 20000\\neps_train, eps_test = 0.1, 0.05\\nstep_per_epoch, step_per_collect = 10000, len(data) * 10\\nlogger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))  # TensorBoard is supported!\\n# For other loggers: https://tianshou.readthedocs.io/en/master/tutorials/logger.html\\n\\n# you can also try with SubprocVectorEnv\\ntrain_envs = ts.env.DummyVectorEnv([lambda: HomerEnv(data=data) for _ in range(train_num)])\\ntest_envs = ts.env.DummyVectorEnv([lambda: HomerEnv(data=data) for _ in range(test_num)])\\n\\nfrom tianshou.utils.net.common import Net\\n# you can define other net by following the API:\\n# https://tianshou.readthedocs.io/en/master/tutorials/dqn.html#build-the-network\\n#env = gym.make('gym_homer/HomerEnv-v0', data=data)\\nwrapped_env = wrappers.FlattenObservation(env)\\nstate_shape = wrapped_env.observation_space.shape or wrapped_env.observation_space.n\\naction_shape = wrapped_env.action_space.shape or wrapped_env.action_space.n\\nnet = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[64, 128, 8])\\noptim = torch.optim.Adam(net.parameters(), lr=lr)\\n\\npolicy = ts.policy.DQNPolicy(net, optim, gamma, n_step, target_update_freq=target_freq)\\ntrain_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(buffer_size, train_num), \\n                                    exploration_noise=True)\\ntest_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)  # because DQN uses epsilon-greedy method\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#task = 'HomerEnv-v0'\n",
    "env = HomerEnv(data=data, start_soc='random')\n",
    "lr, epoch, batch_size = 1e-3, 20, 64\n",
    "train_num, test_num = 10, 100\n",
    "gamma, n_step, target_freq = 0.9, 3, 320\n",
    "buffer_size = 20000\n",
    "eps_train, eps_test = 0.1, 0.05\n",
    "step_per_epoch, step_per_collect = 10000, len(data) * 10\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))  # TensorBoard is supported!\n",
    "# For other loggers: https://tianshou.readthedocs.io/en/master/tutorials/logger.html\n",
    "\n",
    "# you can also try with SubprocVectorEnv\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: HomerEnv(data=data) for _ in range(train_num)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: HomerEnv(data=data) for _ in range(test_num)])\n",
    "\n",
    "from tianshou.utils.net.common import Net\n",
    "# you can define other net by following the API:\n",
    "# https://tianshou.readthedocs.io/en/master/tutorials/dqn.html#build-the-network\n",
    "#env = gym.make('gym_homer/HomerEnv-v0', data=data)\n",
    "wrapped_env = wrappers.FlattenObservation(env)\n",
    "state_shape = wrapped_env.observation_space.shape or wrapped_env.observation_space.n\n",
    "action_shape = wrapped_env.action_space.shape or wrapped_env.action_space.n\n",
    "net = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[64, 128, 8])\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "policy = ts.policy.DQNPolicy(net, optim, gamma, n_step, target_update_freq=target_freq)\n",
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(buffer_size, train_num), \n",
    "                                    exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)  # because DQN uses epsilon-greedy method\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5880876f-d797-4993-b6d7-247ef386a95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'result = ts.trainer.offpolicy_trainer(\\n    policy, train_collector, test_collector, epoch, \\n    step_per_epoch, step_per_collect,\\n    test_num, batch_size, update_per_step=1 / step_per_collect,\\n    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),\\n    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),\\n    stop_fn=lambda mean_rewards: mean_rewards >= 2,\\n    logger=logger)\\nprint(f\\'Finished training! Use {result[\"duration\"]}\\')\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''result = ts.trainer.offpolicy_trainer(\n",
    "    policy, train_collector, test_collector, epoch, \n",
    "    step_per_epoch, step_per_collect,\n",
    "    test_num, batch_size, update_per_step=1 / step_per_collect,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= 2,\n",
    "    logger=logger)\n",
    "print(f'Finished training! Use {result[\"duration\"]}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fe1fca5-3588-425a-ac3d-849538de9e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'policy.eval()\\npolicy.set_eps(20)\\ncollector = ts.data.Collector(policy, env)\\ncollector.collect(n_episode=10)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''policy.eval()\n",
    "policy.set_eps(20)\n",
    "collector = ts.data.Collector(policy, env)\n",
    "collector.collect(n_episode=10)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1dde91",
   "metadata": {},
   "source": [
    "## Continuous action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d23d81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task = 'HomerEnv-v1'\n",
    "env = HomerEnv(data=data, start_soc='random', discrete=False)\n",
    "lr, epoch, batch_size = 1e-3, 20, 64\n",
    "train_num, test_num = 10, 100\n",
    "gamma, n_step, target_freq = 0.9, 3, 320\n",
    "buffer_size = 20000\n",
    "eps_train, eps_test = 0.1, 0.05\n",
    "step_per_epoch, step_per_collect = 10000, len(data) * 10\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))  # TensorBoard is supported!\n",
    "# For other loggers: https://tianshou.readthedocs.io/en/master/tutorials/logger.html\n",
    "\n",
    "# you can also try with SubprocVectorEnv\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: HomerEnv(data=data) for _ in range(train_num)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: HomerEnv(data=data) for _ in range(test_num)])\n",
    "\n",
    "from tianshou.utils.net.common import Net\n",
    "# you can define other net by following the API:\n",
    "# https://tianshou.readthedocs.io/en/master/tutorials/dqn.html#build-the-network\n",
    "#env = gym.make('gym_homer/HomerEnv-v0', data=data)\n",
    "wrapped_env = wrappers.FlattenObservation(env)\n",
    "state_shape = wrapped_env.observation_space.shape or wrapped_env.observation_space.n\n",
    "action_shape = wrapped_env.action_space.shape or wrapped_env.action_space.n\n",
    "net = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[64, 128, 8])\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "policy = ts.policy.DQNPolicy(net, optim, gamma, n_step, target_update_freq=target_freq)\n",
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(buffer_size, train_num), \n",
    "                                    exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)  # because DQN uses epsilon-greedy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7573d14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10080it [00:00, 11938.15it/s, env_step=10080, len=11, loss=0.282, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 10080it [00:00, 10931.57it/s, env_step=20160, len=11, loss=0.008, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 10080it [00:00, 12433.93it/s, env_step=30240, len=11, loss=0.001, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 10080it [00:00, 12459.12it/s, env_step=40320, len=11, loss=0.037, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 10080it [00:00, 12389.46it/s, env_step=50400, len=11, loss=0.066, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 10080it [00:00, 12418.21it/s, env_step=60480, len=11, loss=0.011, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 10080it [00:00, 12433.92it/s, env_step=70560, len=11, loss=0.003, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 10080it [00:00, 12425.15it/s, env_step=80640, len=11, loss=0.016, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 10080it [00:00, 12367.23it/s, env_step=90720, len=11, loss=0.011, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 10080it [00:00, 12421.98it/s, env_step=100800, len=11, loss=0.000, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 10080it [00:00, 12428.79it/s, env_step=110880, len=11, loss=0.000, n/ep=20, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 10080it [00:00, 12462.52it/s, env_step=120960, len=11, loss=0.000, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 10080it [00:00, 12492.25it/s, env_step=131040, len=11, loss=0.000, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 10080it [00:00, 12416.69it/s, env_step=141120, len=11, loss=0.000, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 10080it [00:00, 12485.74it/s, env_step=151200, len=11, loss=0.000, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 10080it [00:00, 12420.46it/s, env_step=161280, len=11, loss=0.000, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 10080it [00:00, 12486.44it/s, env_step=171360, len=11, loss=0.000, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 10080it [00:00, 12457.11it/s, env_step=181440, len=11, loss=0.000, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 10080it [00:00, 12447.37it/s, env_step=191520, len=11, loss=0.000, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 10080it [00:00, 12433.70it/s, env_step=201600, len=11, loss=0.000, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n",
      "Finished training! Use 17.47s\n"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy, train_collector, test_collector, epoch, \n",
    "    step_per_epoch, step_per_collect,\n",
    "    test_num, batch_size, update_per_step=1 / step_per_collect,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= 2,\n",
    "    logger=logger)\n",
    "print(f'Finished training! Use {result[\"duration\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174d2a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brody/school/cap_ve/lib/python3.10/site-packages/tianshou/data/collector.py:68: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n",
      "/Users/brody/school/home-energy-optimizer/gym_homer/envs/test_env_v0.py:302: DeprecationWarning: invalid escape sequence '\\$'\n",
      "  f\"\\nStep Reward: \\${self.reward:.2f},\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "HomerEnv._apply_action() missing 2 required positional arguments: 'home' and 'max_d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m policy\u001b[38;5;241m.\u001b[39mset_eps(\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m      3\u001b[0m collector \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mCollector(policy, env)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/school/cap_ve/lib/python3.10/site-packages/tianshou/data/collector.py:314\u001b[0m, in \u001b[0;36mCollector.collect\u001b[0;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m action_remap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mmap_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mact)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# step in env\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_remap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mready_env_ids\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m    316\u001b[0m     obs_next, rew, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/school/cap_ve/lib/python3.10/site-packages/tianshou/env/venvs.py:284\u001b[0m, in \u001b[0;36mBaseVectorEnv.step\u001b[0;34m(self, action, id)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(action) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mid\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mid\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mid\u001b[39m:\n",
      "File \u001b[0;32m~/school/cap_ve/lib/python3.10/site-packages/tianshou/env/worker/dummy.py:38\u001b[0m, in \u001b[0;36mDummyEnvWorker.send\u001b[0;34m(self, action, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/school/home-energy-optimizer/gym_homer/envs/test_env_v0.py:127\u001b[0m, in \u001b[0;36mHomerEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me_flux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_battery(obs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolar\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[1;32m    122\u001b[0m                                             obs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloads\u001b[39m\u001b[38;5;124m'\u001b[39m]], \n\u001b[1;32m    123\u001b[0m                                             obs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_d\u001b[39m\u001b[38;5;124m'\u001b[39m]], \n\u001b[1;32m    124\u001b[0m                                             obs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_c\u001b[39m\u001b[38;5;124m'\u001b[39m]], \n\u001b[1;32m    125\u001b[0m                                             action)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me_flux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_reward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet,\n\u001b[1;32m    130\u001b[0m                                     obs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexport_tariff\u001b[39m\u001b[38;5;124m'\u001b[39m]], \n\u001b[1;32m    131\u001b[0m                                     obs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimport_tariff\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward \n",
      "\u001b[0;31mTypeError\u001b[0m: HomerEnv._apply_action() missing 2 required positional arguments: 'home' and 'max_d'"
     ]
    }
   ],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(20)\n",
    "collector = ts.data.Collector(policy, env)\n",
    "collector.collect(n_episode=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce32f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "680b5f6c383b0e8429d2a9128e666422e28e4aa5a33b220121c7ea81e5b87d60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
