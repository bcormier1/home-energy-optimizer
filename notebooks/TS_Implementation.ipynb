{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9289f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "parent_dir = os.getcwd()\n",
    "path = os.path.dirname(parent_dir)\n",
    "sys.path.append(path)\n",
    "\n",
    "from gym_homer.envs.test_env_v0 import HomerEnv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gym, torch, numpy as np, torch.nn as nn\n",
    "from tianshou.utils import WandbLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "from gym import spaces, wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25da4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_x</th>\n",
       "      <th>time_y</th>\n",
       "      <th>weekend</th>\n",
       "      <th>month_x</th>\n",
       "      <th>month_y</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>region_3</th>\n",
       "      <th>solar</th>\n",
       "      <th>loads</th>\n",
       "      <th>import_tariff</th>\n",
       "      <th>export_tariff</th>\n",
       "      <th>max_d</th>\n",
       "      <th>max_c</th>\n",
       "      <th>soc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.866</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.866</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.866</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.866</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    time_x  time_y  weekend  month_x  month_y  region_1  region_2  region_3  \\\n",
       "0    0.000   1.000        0      0.0      1.0         0         0         0   \n",
       "1    0.500   0.866        0      0.0      1.0         0         0         0   \n",
       "2    0.866   0.500        0      0.0      1.0         0         0         0   \n",
       "3    1.000   0.000        0      0.0      1.0         0         0         0   \n",
       "4    0.866  -0.500        0      0.0      1.0         0         0         0   \n",
       "5    0.500  -0.866        0      0.0      1.0         0         0         0   \n",
       "6    0.000  -1.000        0      0.0      1.0         0         0         0   \n",
       "7   -0.500  -0.866        0      0.0      1.0         0         0         0   \n",
       "8   -0.866  -0.500        0      0.0      1.0         0         0         0   \n",
       "9   -1.000  -0.000        0      0.0      1.0         0         0         0   \n",
       "10  -0.866   0.500        0      0.0      1.0         0         0         0   \n",
       "11  -0.500   0.866        0      0.0      1.0         0         0         0   \n",
       "\n",
       "    solar  loads  import_tariff  export_tariff  max_d  max_c  soc  \n",
       "0       0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "1      -2    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "2      -2    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "3       0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "4       0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "5       0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "6       0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "7       0    0.5           2.00            2.0    0.0    0.0  0.0  \n",
       "8       0    0.5           2.00            2.0    0.0    0.0  0.0  \n",
       "9       0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "10      0    0.5           0.05            0.0    0.0    0.0  0.0  \n",
       "11      0    0.5           0.05            0.0    0.0    0.0  0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "data = pd.read_csv(path+\"/test_env_data.csv\", index_col=False).fillna(0)\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7baa9849-98c7-4863-ae11-abe60bdeaa4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#task = 'HomerEnv-v0'\\nenv = HomerEnv(data=data, start_soc='random')\\nlr, epoch, batch_size = 1e-3, 20, 64\\ntrain_num, test_num = 10, 100\\ngamma, n_step, target_freq = 0.9, 3, 320\\nbuffer_size = 20000\\neps_train, eps_test = 0.1, 0.05\\nstep_per_epoch, step_per_collect = 10000, len(data) * 10\\nlogger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))  # TensorBoard is supported!\\n# For other loggers: https://tianshou.readthedocs.io/en/master/tutorials/logger.html\\n\\n# you can also try with SubprocVectorEnv\\ntrain_envs = ts.env.DummyVectorEnv([lambda: HomerEnv(data=data) for _ in range(train_num)])\\ntest_envs = ts.env.DummyVectorEnv([lambda: HomerEnv(data=data) for _ in range(test_num)])\\n\\nfrom tianshou.utils.net.common import Net\\n# you can define other net by following the API:\\n# https://tianshou.readthedocs.io/en/master/tutorials/dqn.html#build-the-network\\n#env = gym.make('gym_homer/HomerEnv-v0', data=data)\\nwrapped_env = wrappers.FlattenObservation(env)\\nstate_shape = wrapped_env.observation_space.shape or wrapped_env.observation_space.n\\naction_shape = wrapped_env.action_space.shape or wrapped_env.action_space.n\\nnet = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[64, 128, 8])\\noptim = torch.optim.Adam(net.parameters(), lr=lr)\\n\\npolicy = ts.policy.DQNPolicy(net, optim, gamma, n_step, target_update_freq=target_freq)\\ntrain_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(buffer_size, train_num), \\n                                    exploration_noise=True)\\ntest_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)  # because DQN uses epsilon-greedy method\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#task = 'HomerEnv-v0'\n",
    "env = HomerEnv(data=data, start_soc='random')\n",
    "lr, epoch, batch_size = 1e-3, 20, 64\n",
    "train_num, test_num = 10, 100\n",
    "gamma, n_step, target_freq = 0.9, 3, 320\n",
    "buffer_size = 20000\n",
    "eps_train, eps_test = 0.1, 0.05\n",
    "step_per_epoch, step_per_collect = 10000, len(data) * 10\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))  # TensorBoard is supported!\n",
    "# For other loggers: https://tianshou.readthedocs.io/en/master/tutorials/logger.html\n",
    "\n",
    "# you can also try with SubprocVectorEnv\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: HomerEnv(data=data) for _ in range(train_num)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: HomerEnv(data=data) for _ in range(test_num)])\n",
    "\n",
    "from tianshou.utils.net.common import Net\n",
    "# you can define other net by following the API:\n",
    "# https://tianshou.readthedocs.io/en/master/tutorials/dqn.html#build-the-network\n",
    "#env = gym.make('gym_homer/HomerEnv-v0', data=data)\n",
    "wrapped_env = wrappers.FlattenObservation(env)\n",
    "state_shape = wrapped_env.observation_space.shape or wrapped_env.observation_space.n\n",
    "action_shape = wrapped_env.action_space.shape or wrapped_env.action_space.n\n",
    "net = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[64, 128, 8])\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "policy = ts.policy.DQNPolicy(net, optim, gamma, n_step, target_update_freq=target_freq)\n",
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(buffer_size, train_num), \n",
    "                                    exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)  # because DQN uses epsilon-greedy method\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5880876f-d797-4993-b6d7-247ef386a95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'result = ts.trainer.offpolicy_trainer(\\n    policy, train_collector, test_collector, epoch, \\n    step_per_epoch, step_per_collect,\\n    test_num, batch_size, update_per_step=1 / step_per_collect,\\n    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),\\n    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),\\n    stop_fn=lambda mean_rewards: mean_rewards >= 2,\\n    logger=logger)\\nprint(f\\'Finished training! Use {result[\"duration\"]}\\')\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''result = ts.trainer.offpolicy_trainer(\n",
    "    policy, train_collector, test_collector, epoch, \n",
    "    step_per_epoch, step_per_collect,\n",
    "    test_num, batch_size, update_per_step=1 / step_per_collect,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= 2,\n",
    "    logger=logger)\n",
    "print(f'Finished training! Use {result[\"duration\"]}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fe1fca5-3588-425a-ac3d-849538de9e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'policy.eval()\\npolicy.set_eps(20)\\ncollector = ts.data.Collector(policy, env)\\ncollector.collect(n_episode=10)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''policy.eval()\n",
    "policy.set_eps(20)\n",
    "collector = ts.data.Collector(policy, env)\n",
    "collector.collect(n_episode=10)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230a6cb",
   "metadata": {},
   "source": [
    "## Continuous action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3e1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task = 'HomerEnv-v1'\n",
    "env = HomerEnv(data=data, start_soc='random', discrete=False)\n",
    "lr, epoch, batch_size = 1e-3, 10, 64\n",
    "train_num, test_num = 10, 100\n",
    "gamma, n_step, target_freq = 0.9, 3, 320\n",
    "buffer_size = 20000\n",
    "eps_train, eps_test = 0.1, 0.05\n",
    "step_per_epoch, step_per_collect = 10000, len(data) * 10\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))  # TensorBoard is supported!\n",
    "# For other loggers: https://tianshou.readthedocs.io/en/master/tutorials/logger.html\n",
    "\n",
    "# you can also try with SubprocVectorEnv\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: HomerEnv(data=data) for _ in range(train_num)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: HomerEnv(data=data) for _ in range(test_num)])\n",
    "\n",
    "from tianshou.utils.net.common import Net\n",
    "# you can define other net by following the API:\n",
    "# https://tianshou.readthedocs.io/en/master/tutorials/dqn.html#build-the-network\n",
    "#env = gym.make('gym_homer/HomerEnv-v0', data=data)\n",
    "wrapped_env = wrappers.FlattenObservation(env)\n",
    "state_shape = wrapped_env.observation_space.shape or wrapped_env.observation_space.n\n",
    "action_shape = wrapped_env.action_space.shape or wrapped_env.action_space.n\n",
    "net = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[64, 128, 8])\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "policy = ts.policy.DQNPolicy(net, optim, gamma, n_step, target_update_freq=target_freq)\n",
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(buffer_size, train_num), \n",
    "                                    exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)  # because DQN uses epsilon-greedy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2715e9eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10080it [00:00, 11991.09it/s, env_step=10080, len=11, loss=0.289, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 10080it [00:00, 12434.77it/s, env_step=20160, len=11, loss=0.018, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 10080it [00:00, 12342.52it/s, env_step=30240, len=11, loss=0.004, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 10080it [00:00, 12370.92it/s, env_step=40320, len=11, loss=0.053, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 10080it [00:00, 12350.61it/s, env_step=50400, len=11, loss=0.074, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 10080it [00:00, 12494.48it/s, env_step=60480, len=11, loss=0.007, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 10080it [00:00, 12406.50it/s, env_step=70560, len=11, loss=0.004, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 10080it [00:00, 12430.82it/s, env_step=80640, len=11, loss=0.017, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 10080it [00:00, 12399.92it/s, env_step=90720, len=11, loss=0.007, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 10080it [00:00, 12059.62it/s, env_step=100800, len=11, loss=0.000, n/ep=10, n/st=120, rew=-2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: -2.175000 ± 0.000000, best_reward: -2.175000 ± 0.000000 in #0\n",
      "Finished training! Use 8.76s\n"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy, train_collector, test_collector, epoch, \n",
    "    step_per_epoch, step_per_collect,\n",
    "    test_num, batch_size, update_per_step=1 / step_per_collect,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= 2,\n",
    "    logger=logger)\n",
    "print(f'Finished training! Use {result[\"duration\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa3498b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brody/school/cap_ve/lib/python3.10/site-packages/tianshou/data/collector.py:68: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n/ep': 10,\n",
       " 'n/st': 110,\n",
       " 'rews': array([-2.175, -2.175, -2.175, -2.175, -2.175, -2.175, -2.175, -2.175,\n",
       "        -2.175, -2.175]),\n",
       " 'lens': array([11, 11, 11, 11, 11, 11, 11, 11, 11, 11]),\n",
       " 'idxs': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'rew': -2.175000002607703,\n",
       " 'len': 11.0,\n",
       " 'rew_std': 0.0,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(20)\n",
    "collector = ts.data.Collector(policy, env)\n",
    "collector.collect(n_episode=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dbae95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "680b5f6c383b0e8429d2a9128e666422e28e4aa5a33b220121c7ea81e5b87d60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
