program: ppo_experiment_v1.py
command:
  - ${env}
  - python
  - ${program}
  - "--file"
  - "config/settings_ppo.json"
  - "--sweep"
method: bayes
metric:
  goal: maximize
  name: train/reward
parameters:
  action_intervals:
    values: [1, 5, 10, 20]
  hidden_layer_dims:
    values: [32, 64, 128, 512]
  lr_decay:
    distribution: categorical
    values:
      - "true"
      - "false"
  lr_optimizer:
    distribution: uniform
    max: 0.0005
    min: 0.0000005
  n_hidden_layers:
    distribution: int_uniform
    max: 4
    min: 2
  replay_buffer_collector:
    values: [1e6, 2e6, 3e6]
  episode_length:
    distribution: int_uniform
    max: 432
    min: 144
  icm:
    distribution: categorical
    values:
      - "true"
      - "false"
  prioritized_replay_buffer:
    distribution: categorical
    values:
      - "true"
      - "false"
