program: ppo_experiment_v1.py
command:
  - ${env}
  - python
  - ${program}
  - "--file"
  - "config/settings_ppo.json"
  - "--sweep"
method: bayes
metric:
  goal: maximize
  name: train/reward
parameters:
  action_intervals:
    values: [1, 5, 10, 20]
  hidden_layer_dims:
    values: [32, 64, 128, 512]
  lr_decay:
    distribution: categorical
    values:
      - "true"
      - "false"
  lr_optimizer:
    distribution: uniform
    max: 0.001
    min: 0.0000005
  replay_buffer_collector:
    values: [1e6, 2e6, 3e6]
  icm:
    distribution: categorical
    values:
      - "true"
      - "false"
  prioritized_replay_buffer:
    distribution: categorical
    values:
      - "true"
      - "false"
  eps_clip:
    distribution: uniform
    max: 0.3
    min: 0.1
  repeat_per_collector:
    values: [1, 2, 4]
